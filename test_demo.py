"""
Script demo nhanh ƒë·ªÉ test c√°c module
"""

def test_imports():
    """Test import c√°c module"""
    print("üß™ Ki·ªÉm tra imports...")
    
    try:
        from nlp_toolkit import (
            DataCleaner, TextParser, BagOfWordsEncoder, 
            TfidfEncoder, Word2VecEncoder, CharNgramEncoder
        )
        print("‚úÖ nlp_toolkit import OK")
    except Exception as e:
        print(f"‚ùå nlp_toolkit import FAILED: {e}")
        return False
    
    try:
        from ml_engine import MLTrainer, PerformanceEvaluator, ExperimentLogger
        print("‚úÖ ml_engine import OK")
    except Exception as e:
        print(f"‚ùå ml_engine import FAILED: {e}")
        return False
    
    return True


def test_text_cleaning():
    """Test text cleaning"""
    print("\nüß™ Ki·ªÉm tra text cleaning...")
    
    from nlp_toolkit import DataCleaner
    
    test_text = "Hello WORLD!!! Visit https://example.com for more info... 12345"
    cleaned = DataCleaner.sanitize_content(test_text)
    
    print(f"Original: {test_text}")
    print(f"Cleaned:  {cleaned}")
    print("‚úÖ Text cleaning OK")


def test_tokenization():
    """Test tokenization"""
    print("\nüß™ Ki·ªÉm tra tokenization...")
    
    from nlp_toolkit import TextParser
    
    text = "this is a test sentence"
    tokens = TextParser.extract_tokens(text)
    count = TextParser.get_token_count(text)
    
    print(f"Text:   {text}")
    print(f"Tokens: {tokens}")
    print(f"Count:  {count}")
    print("‚úÖ Tokenization OK")


def test_encoders():
    """Test c√°c encoder"""
    print("\nüß™ Ki·ªÉm tra encoders...")
    
    from nlp_toolkit import BagOfWordsEncoder, TfidfEncoder, CharNgramEncoder
    
    corpus = [
        "this is a test",
        "another test document",
        "third document here"
    ]
    
    # Test BoW
    bow = BagOfWordsEncoder(max_vocab=50)
    bow_features = bow.fit_transform(corpus)
    print(f"‚úÖ BoW: shape={bow_features.shape}, name={bow.get_name()}")
    
    # Test TF-IDF
    tfidf = TfidfEncoder(max_vocab=50)
    tfidf_features = tfidf.fit_transform(corpus)
    print(f"‚úÖ TF-IDF: shape={tfidf_features.shape}, name={tfidf.get_name()}")
    
    # Test CharNgram
    char_ngram = CharNgramEncoder(max_features=100, ngram_range=(2, 3))
    char_features = char_ngram.fit_transform(corpus)
    print(f"‚úÖ CharNgram: shape={char_features.shape}, name={char_ngram.get_name()}")


def test_ml_trainer():
    """Test ML trainer"""
    print("\nüß™ Ki·ªÉm tra ML trainer...")
    
    from ml_engine import MLTrainer
    
    trainer = MLTrainer()
    print(f"‚úÖ MLTrainer kh·ªüi t·∫°o th√†nh c√¥ng v·ªõi {len(trainer.algorithms)} thu·∫≠t to√°n")
    
    for algo_name in trainer.algorithms.keys():
        print(f"   - {algo_name}")


def main():
    """Ch·∫°y t·∫•t c·∫£ c√°c test"""
    print("=" * 70)
    print("üöÄ DEMO & TEST NLP TOOLKIT")
    print("=" * 70)
    
    if not test_imports():
        print("\n‚ùå Import failed! Ki·ªÉm tra l·∫°i dependencies.")
        return
    
    test_text_cleaning()
    test_tokenization()
    test_encoders()
    test_ml_trainer()
    
    print("\n" + "=" * 70)
    print("‚úÖ T·∫§T C·∫¢ TEST HO√ÄN T·∫§T!")
    print("=" * 70)
    print("\nüí° B√¢y gi·ªù b·∫°n c√≥ th·ªÉ:")
    print("   1. Ch·∫°y: python convert_data.py (n·∫øu c√≥ file JSON)")
    print("   2. M·ªü: jupyter notebook training_pipeline.ipynb")
    print("   3. Ho·∫∑c import v√† s·ª≠ d·ª•ng trong code c·ªßa b·∫°n")


if __name__ == "__main__":
    main()
